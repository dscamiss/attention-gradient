# attention-gradient

A no-frills PyTorch implementation of scaled dot-product attention,
with an explicit `backward()` method to compute gradients.

The implementation follows this blog post: https://dscamiss.github.io/blog/posts/attention-explicit/.