# attention-gradient
Scaled dot-product attention with an explicit backward() method
